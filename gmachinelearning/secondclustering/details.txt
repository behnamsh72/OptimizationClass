Clustering is an Optimization Problem:
variability of a cluster:
variability(c)=sigma(e member of cluster) distance(mean(cluster),e)**2

dissimilarity of set of clusters:
dissimilarity(C)=sigma(c member of clusters)variability(c)


variability it's not quite variance but it's very close to it.

If i divide the variability be the size of the cluster I would have Variance.
we divide variability to size of the cluster to normalize this.

Can we say that the Optimization problem we want to solve by clustering
is simply finding a capital C that minimizes dissimilarity?No
We could constraint the number of clusters,for Example I only want to have
at most five clusters.Do the best we can to minimize dissimilarity but we're not
allowed to use more than five clusters.That's the most common constraint
that gets places in the problem.

There are two methods(with many implementation):
The first is called hierarchical clustering and the second calls K-Means.

Hierarchical Clustering:
1-Start by assigning each item to a cluster, so that if we have N items,we now have
N clusters, each containing just one item.

2-Find the closest(most similar) pair of clusters and merge them into a single
cluster,so that now we have one fewer cluster.

3-Continue the process until all items are clustered into a single cluster
of size N.

what's interesting about hierarchical clustering is we stop it typically somewhere along the way

We can have some stopping criteria.


This is called agglomerative hierarchical clustering  because we start with bunch of thing and we agglomerate them or we put them together.

What does distance mean?

Linkage Metrics:
1-Single-linkage:consider the distance between one cluster and another cluster
to be equal to the shortest distance from any member of cluster to any member of
the other cluster.

2-Complete-linkage:
consider the distance between any two clusters is equal to the greatest
distance from any member to any other member.

3-Average-linkage:
consider the distance between one cluster and another cluster to be equal to the
average distance from any member of one cluster to any member of the other cluster.

They're all used in practiced for different kinds of results depending on the
application of the clustering.


Given a linkage criterion, we always get the same answer.There's nothing
random here.it's completely deterministic.
Notice by the way the answer might not be optimal with regards to that linkage
criteria.It's a greedy algorithm exactly.we're making locally optimum decision
at which point which may or may  not be globally optimal.it's flexible,
choosing different linkage criteria i get different results.But it's also
potentially really really slow.This is not something we want to do on a milion
examples.

◦ Naïve algorithm n^3
◦ n^2 algorithms exist for some linkage criteria for example single-linage

K-means a much faster greedy algorithm
Most useful when you know how many clusters you want
K is the number of clusters we want.

K-means Algorithms:
randomly chose k examples as initial centroids.
while true:
    create k clusters by assigning each example to closest centroid.
    compute k new centroids by averaging examples in each cluster
    if centroid don't change:
        break
So it's converged.

what's the complexity of 1 iteration:
we've got k centroids.Now i have to take each example and compare it to each in a naively,at least to each centroid to see which is closest to.
so that's k comparisons per example.
k*n*d, where n is number of points and d time required
to compute the distance between a pair of points

This is a way small number than N squared,typically so each iteration is pretty quick,
as we'll see, this typically converges quite quickly so we usually need a very small number of iterations.
so it's quite efficient and there are various ways we can optimize it.to make it even more efficient.
this is the most commonly-used clustering algorithm because it works really fast.

PROBLEMS WITH K MEANS:
1-how do we choose k?
2-the results can depend upon the initial centroids.unlike hierarchical clustering,
k means is non deterministic.Depending upon what random examples we choose,
we can get a different number of iterations.

let's think about choosing k:
people choosing k using a priori knowledge about the application.
often, we know enough about the application, we can choose k.
A better approach is to search for a good k.so we can try different values of k and evaluate the quality of the result.
or we can run hierarchical clustering on a subset of data.I've got a milion points,what I'm going to do is take a subset
of 10000, run hierarchical clustering From that get a sense of the structure underlying the data.
Decide k should be 6,and then run k-means with k=6, people often do this.They run hierarchical clustering on a small
subset of the data and then choose k.

What about unlucky centroids:
We could be clever and try and select good initial centroids.So people often will do that,
and what they'll do is try and just make sure that they're distributed over the space
Another approach is to try multiple sets of randomly chosen centroids, and then just select the best results.